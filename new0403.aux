\relax 
\citation{satchler1990introduction}
\citation{brown2018endf}
\citation{blatt1952angular,lane1958r}
\citation{larson1998updated,larson1998updated}
\citation{kunieda2014r,kunieda2015covariance}
\citation{Rochman2013}
\citation{nobre2023novel}
\citation{chen1990r,an2015astrophysical}
\citation{ge2020cendl}
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Phase Shift Deep Neural Network Approach for Studying Resonance Cross Sections of $^{235}$U$(n, f)$ Reaction}{1}{}}
\@writefile{toc}{\contentsline {abstract}{摘要}{1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {I}introduction}{1}{}}
\citation{boehnlein2022colloquium,李庆峰2022机器学习在原子核物理中的应用专题,nucleartech2023,ma2023phase,he2023machine,he2023high}
\citation{Niu2018,xing2023study}
\citation{shang2022prediction}
\citation{saxena2021modified}
\citation{minato2022calculation,niu2019comparative}
\citation{wang2021optimizing}
\citation{Akkoyun2020,胡泽华2023深度神经网络学习快中子截面}
\citation{vicente2021nuclear}
\citation{nobre2023novel}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theoretical Framework}{2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Deep Neural Network}{2}{}}
\citation{kingma2014adam}
\citation{xu2019frequency}
\citation{cai2019phasednn}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The comparison of the raw cross sections and the theoretical results predicted by DNNs for $^{235}$U$(n, f)$ reaction. Preprocessing is performed by taking the input quantities and labels as logarithms. The black points in the figure are the evaluated data derived from ENDF/B-VIII.0. The blue and the red lines are the theoretical results predicted by DNN with the Tanh and Relu activation functions, respectively. It can be seen that DNNs can reasonable reproduce the general trend of the excitation function. However, no matter what activation function is used, DNNs cannot reproduce the evaluated data for the high frequency resonance region.}}{3}{}}
\newlabel{u235fp}{{1}{3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The training loss of DNN using the tanh activation function (blue line) and the relu activation function (red line). }}{3}{}}
\newlabel{fig:u235fploss1}{{2}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Parallel Phase Shift Deep Neural Network}{4}{}}
\newlabel{f1(x)}{{1}{4}{}{}{}}
\newlabel{eq:1psdnn}{{2}{4}{}{}{}}
\newlabel{rj(x)}{{4}{4}{}{}{}}
\newlabel{convol}{{5}{4}{}{}{}}
\newlabel{rjshift}{{6}{4}{}{}{}}
\citation{cai2020phase}
\newlabel{DNNupdate}{{9}{5}{}{}{}}
\newlabel{DNNupdate}{{10}{5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces PPSDNN neural network structure diagram. Where $T_j(x) (j = 0, 1, ..., m)$ represents one training process of the neural network.}}{5}{}}
\newlabel{ppsdnnchart1}{{3}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Coupled Phase Shift Deep Neural Network}{5}{}}
\newlabel{eq:CPDNNcomplex}{{11}{5}{}{}{}}
\newlabel{eq:CPDNNreal}{{12}{5}{}{}{}}
\newlabel{eq:2}{{13}{5}{}{}{}}
\newlabel{eq:Ln2}{{14}{5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The neural network structure diagram of CPSDNN. Each $F_{m}(x) (m = 0, 1, ..., m)$ represents a neural network. The $T(x$) represents the training process of neural network.}}{5}{}}
\newlabel{ppsdnnchart2}{{4}{5}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Application to $^{235}\text  {U}(n, f)$ reaction}{6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The comparison between the raw resonance data (black dots) and the predicted results (red line) of CPSDNN. The raw resonance cross section derived fron ENDF/B-III.0 for $^{235}$U(n,f). }}{6}{}}
\newlabel{endftrain}{{5}{6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $1\times 10^{-8}$ MeV to $1\times 10^{-6}$ MeV.}}{6}{}}
\newlabel{}{{6}{6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $1\times 10^{-6}$ MeV to $1\times 10^{-5}$ MeV.}}{6}{}}
\newlabel{}{{7}{6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $1\times 10^{-5}$ MeV to $1\times 10^{-4}$ MeV.}}{7}{}}
\newlabel{5-4}{{8}{7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Sammary}{7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $ 10^{-4}$ MeV to $6\times 10^{-4}$ MeV.}}{7}{}}
\newlabel{1E-4}{{9}{7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $6\times 10^{-4}$ MeV to $ 10^{-3}$ MeV.}}{8}{}}
\newlabel{1E-4}{{10}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $10^{-3}$ MeV to $ 10^{-2}$ MeV.}}{8}{}}
\newlabel{}{{11}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $ 10^{-2}$ MeV to $ 10^{-1}$ MeV.}}{8}{}}
\newlabel{}{{12}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The same as Fig. 5{}{}{}\hbox {} but for the energy range only from $1$ MeV to $ 30$ MeV.}}{8}{}}
\newlabel{}{{13}{8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The training loss of CPSDNN (black line) for ENDF/B-III.0 data. The training losses of DNN using the tanh activation function (blue line) and the relu activation function (red line) are same as Fig. 2{}{}{}\hbox {} but for logarithmic scale and only to 70 epoch here.}}{8}{}}
\newlabel{loss1}{{14}{8}{}{}{}}
\bibdata{new0403Notes,PRCreferences}
\bibcite{satchler1990introduction}{{1}{1990}{{Satchler\ and\ Satchler}}{{}}}
\bibcite{brown2018endf}{{2}{2018}{{Brown\ \emph  {et~al.}}}{{Brown, Chadwick, Capote, Kahler, Trkov, Herman, Sonzogni, Danon, Carlson, Dunn \emph  {et~al.}}}}
\bibcite{blatt1952angular}{{3}{1952}{{Blatt\ and\ Biedenharn}}{{}}}
\bibcite{lane1958r}{{4}{1958}{{Lane\ and\ Thomas}}{{}}}
\bibcite{larson1998updated}{{5}{1998}{{Larson}}{{}}}
\bibcite{kunieda2014r}{{6}{2014}{{Kunieda\ \emph  {et~al.}}}{{Kunieda, Kawano, Paris, Hale, Shibata,\ and\ Fukahori}}}
\bibcite{kunieda2015covariance}{{7}{2015}{{Kunieda\ \emph  {et~al.}}}{{Kunieda, Kawano, Paris, Hale, Shibata,\ and\ Fukahori}}}
\bibcite{Rochman2013}{{8}{2013}{{Rochman\ \emph  {et~al.}}}{{Rochman, Koning, Kopecky, Sublet, Ribon,\ and\ Moxon}}}
\bibcite{nobre2023novel}{{9}{2023}{{Nobre\ \emph  {et~al.}}}{{Nobre, Brown, Hollick, Scoville,\ and\ Rodr{\'\i }guez}}}
\bibcite{chen1990r}{{10}{1990}{{Chen\ and\ Qi}}{{}}}
\bibcite{an2015astrophysical}{{11}{2015}{{An\ \emph  {et~al.}}}{{An, Chen, Ma, Yu, Sun, Fan, Li, Xu, Huang,\ and\ Wang}}}
\bibcite{ge2020cendl}{{12}{2020}{{Ge\ \emph  {et~al.}}}{{Ge, Xu, Wu, Zhang, Chen, Jin, Shu, Chen, Tao, Tian \emph  {et~al.}}}}
\bibcite{boehnlein2022colloquium}{{13}{2022}{{Boehnlein\ \emph  {et~al.}}}{{Boehnlein, Diefenthaler, Sato, Schram, Ziegler, Fanelli, Hjorth-Jensen, Horn, Kuchera, Lee \emph  {et~al.}}}}
\bibcite{李庆峰2022机器学习在原子核物理中的应用专题}{{14}{2022}{{李庆峰\ and\ 马余刚}}{{}}}
\bibcite{nucleartech2023}{{15}{2023}{{Ma~Yugang\ and\ Yingxun}}{{}}}
\bibcite{ma2023phase}{{16}{2023}{{Ma\ \emph  {et~al.}}}{{Ma, Pang, Wang,\ and\ Zhou}}}
\bibcite{he2023machine}{{17}{2023{}}{{He\ \emph  {et~al.}}}{{He, Li, Ma, Niu, Pei,\ and\ Zhang}}}
\bibcite{he2023high}{{18}{2023{}}{{He\ \emph  {et~al.}}}{{He, Ma, Pang, Song,\ and\ Zhou}}}
\bibcite{Niu2018}{{19}{2018}{{Niu\ and\ Liang}}{{}}}
\bibcite{xing2023study}{{20}{2023}{{XING\ \emph  {et~al.}}}{{XING, LIANG,\ and\ SUN}}}
\bibcite{shang2022prediction}{{21}{2022}{{Shang\ \emph  {et~al.}}}{{Shang, Li,\ and\ Niu}}}
\bibcite{saxena2021modified}{{22}{2021}{{Saxena\ \emph  {et~al.}}}{{Saxena, Sharma,\ and\ Saxena}}}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgments}{9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{9}{}}
\bibcite{minato2022calculation}{{23}{2022}{{Minato\ \emph  {et~al.}}}{{Minato, Niu, Liang \emph  {et~al.}}}}
\bibcite{niu2019comparative}{{24}{2019}{{Niu\ \emph  {et~al.}}}{{Niu, Fang, Niu \emph  {et~al.}}}}
\bibcite{wang2021optimizing}{{25}{2021}{{Wang\ and\ Pei}}{{}}}
\bibcite{Akkoyun2020}{{26}{2020}{{Akkoyun}}{{}}}
\bibcite{胡泽华2023深度神经网络学习快中子截面}{{27}{2023}{{胡泽华\ \emph  {et~al.}}}{{胡泽华, 应阳君, 勇珩,\ and\ 续瑞瑞}}}
\bibcite{vicente2021nuclear}{{28}{2021}{{Vicente-Valdez\ \emph  {et~al.}}}{{Vicente-Valdez, Bernstein,\ and\ Fratoni}}}
\bibcite{kingma2014adam}{{29}{2014}{{Kingma\ and\ Ba}}{{}}}
\bibcite{xu2019frequency}{{30}{2019}{{Xu\ \emph  {et~al.}}}{{Xu, Zhang, Luo, Xiao,\ and\ Ma}}}
\bibcite{cai2019phasednn}{{31}{2019}{{Cai\ \emph  {et~al.}}}{{Cai, Li,\ and\ Liu}}}
\bibcite{cai2020phase}{{32}{2020}{{Cai\ \emph  {et~al.}}}{{Cai, Li,\ and\ Liu}}}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\newlabel{LastBibItem}{{32}{10}{}{}{}}
\newlabel{LastPage}{{}{10}{}{}{}}
\gdef \@abspage@last{10}
